{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74893ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    mean_squared_error, r2_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362e3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_cme_multifeature_from_flatdir(year: int, cme_data_dir: str) -> pd.DataFrame:\n",
    "    cme_data_dir = Path(cme_data_dir)\n",
    "    files = [cme_data_dir / f\"univ{year}_{m:02d}.txt\" for m in range(1, 13)]\n",
    "\n",
    "    full_dates = pd.date_range(pd.Timestamp(year, 1, 1), pd.Timestamp(year, 12, 31), freq=\"D\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        if not f.exists():\n",
    "            continue\n",
    "        df = pd.read_fwf(f, skiprows=4, header=None)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # 해당 연도 CME 파일이 하나도 없으면 0으로 채운 daily 리턴\n",
    "    if not dfs:\n",
    "        daily = pd.DataFrame({\"date\": full_dates})\n",
    "        for c in [\n",
    "            \"cme_count\",\"halo_count\",\"partial_halo_count\",\n",
    "            \"width_max\",\"width_mean\",\n",
    "            \"speed_linear_max\",\"speed_linear_mean\",\n",
    "            \"speed_init_max\",\"speed_final_max\",\"speed_20R_max\",\n",
    "            \"accel_max\",\"accel_min\",\n",
    "            \"mass_sum\",\"mass_max\",\n",
    "            \"ke_sum\",\"ke_max\"\n",
    "        ]:\n",
    "            daily[c] = 0.0\n",
    "        return daily\n",
    "\n",
    "    cme = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # 열이 부족하면 채워넣기(포맷 변형 방어)\n",
    "    if cme.shape[1] < 13:\n",
    "        for _ in range(13 - cme.shape[1]):\n",
    "            cme[cme.shape[1]] = np.nan\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"date\"] = pd.to_datetime(cme.iloc[:, 0], errors=\"coerce\")\n",
    "\n",
    "    central = cme.iloc[:, 2].astype(str)\n",
    "    out[\"is_halo\"] = central.str.contains(\"Halo\", case=False, na=False).astype(int)\n",
    "    out[\"is_partial_halo\"] = cme.iloc[:, 12].astype(str).str.contains(\"Partial\", case=False, na=False).astype(int)\n",
    "\n",
    "    def num(col):\n",
    "        return pd.to_numeric(\n",
    "            cme.iloc[:, col].astype(str).str.replace(\"*\", \"\", regex=False),\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    out[\"width\"] = num(3)\n",
    "    out[\"speed_linear\"] = num(4)\n",
    "    out[\"speed_init\"] = num(5)\n",
    "    out[\"speed_final\"] = num(6)\n",
    "    out[\"speed_20R\"] = num(7)\n",
    "    out[\"accel\"] = num(8)\n",
    "    out[\"mass\"] = num(9)\n",
    "    out[\"kinetic_energy\"] = num(10)\n",
    "\n",
    "    out = out.dropna(subset=[\"date\"])\n",
    "\n",
    "    daily = (\n",
    "        out.groupby(\"date\")\n",
    "        .agg(\n",
    "            cme_count=(\"date\", \"count\"),\n",
    "            halo_count=(\"is_halo\", \"sum\"),\n",
    "            partial_halo_count=(\"is_partial_halo\", \"sum\"),\n",
    "\n",
    "            width_max=(\"width\", \"max\"),\n",
    "            width_mean=(\"width\", \"mean\"),\n",
    "\n",
    "            speed_linear_max=(\"speed_linear\", \"max\"),\n",
    "            speed_linear_mean=(\"speed_linear\", \"mean\"),\n",
    "\n",
    "            speed_init_max=(\"speed_init\", \"max\"),\n",
    "            speed_final_max=(\"speed_final\", \"max\"),\n",
    "            speed_20R_max=(\"speed_20R\", \"max\"),\n",
    "\n",
    "            accel_max=(\"accel\", \"max\"),\n",
    "            accel_min=(\"accel\", \"min\"),\n",
    "\n",
    "            mass_sum=(\"mass\", \"sum\"),\n",
    "            mass_max=(\"mass\", \"max\"),\n",
    "\n",
    "            ke_sum=(\"kinetic_energy\", \"sum\"),\n",
    "            ke_max=(\"kinetic_energy\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # 연도 전체 날짜로 채우기\n",
    "    daily = (\n",
    "        daily.set_index(\"date\")\n",
    "        .reindex(full_dates)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # NaN -> 0\n",
    "    daily = daily.fillna(0.0)\n",
    "\n",
    "    return daily\n",
    "\n",
    "# -----------------------------\n",
    "# hproton (DPD): 파서 에러/메타라인 방어 로더\n",
    "# -----------------------------\n",
    "def load_and_preprocess_hproton_from_dir(year: int, hproton_data_dir: str) -> pd.DataFrame:\n",
    "    dpd_path = Path(hproton_data_dir) / f\"{year}_DPD.txt\"\n",
    "    if not dpd_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing DPD file: {dpd_path}\")\n",
    "\n",
    "    rows = []\n",
    "    with open(dpd_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 6:\n",
    "                continue\n",
    "            try:\n",
    "                y = int(float(parts[0])); m = int(float(parts[1])); d = int(float(parts[2]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            vals = parts[:9] + [np.nan] * (9 - len(parts[:9]))\n",
    "            rows.append([y, m, d] + vals[3:9])\n",
    "\n",
    "    hp = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"year\",\"month\",\"day\",\n",
    "            \"p_gt_1MeV\",\"p_gt_10MeV\",\"p_gt_100MeV\",\n",
    "            \"e_gt_0p6MeV\",\"e_gt_2MeV\",\n",
    "            \"neutron_pct\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hp[\"p_gt_100MeV\"] = pd.to_numeric(hp[\"p_gt_100MeV\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "    hp[\"p_gt_100MeV\"] = hp[\"p_gt_100MeV\"].where(hp[\"p_gt_100MeV\"] >= 0)\n",
    "    hp[\"p_gt_100MeV\"] = hp[\"p_gt_100MeV\"].fillna(0.0)\n",
    "    hp[\"date\"] = pd.to_datetime(hp[[\"year\",\"month\",\"day\"]])\n",
    "\n",
    "    return hp[[\"date\",\"p_gt_100MeV\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 여러 해를 합쳐서 연속 날짜축으로 align\n",
    "# -----------------------------\n",
    "def build_merged_all_years(start_year=1996, end_year=2018,\n",
    "                           cme_dir=\"./cme_data\", hproton_dir=\"./hproton_data\"):\n",
    "    cme_all = []\n",
    "    hp_all = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # CME (해당 연도 파일이 없으면 skip)\n",
    "        try:\n",
    "            cme_daily = load_and_preprocess_cme_multifeature(year, cme_dir)\n",
    "            cme_all.append(cme_daily)\n",
    "        except ValueError:\n",
    "            # pd.concat에 들어갈 파일이 없어서 생기는 경우 등\n",
    "            continue\n",
    "\n",
    "        # DPD (없으면 skip)\n",
    "        dpd_path = f\"{hproton_dir}/{year}_DPD.txt\"\n",
    "        try:\n",
    "            hp_daily = load_and_preprocess_hproton(dpd_path)\n",
    "            hp_all.append(hp_daily)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    cme_all = pd.concat(cme_all, ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "    hp_all  = pd.concat(hp_all,  ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # 전체 기간 공통 날짜로 맞추기\n",
    "    start = max(cme_all[\"date\"].min(), hp_all[\"date\"].min())\n",
    "    end   = min(cme_all[\"date\"].max(), hp_all[\"date\"].max())\n",
    "    full_dates = pd.date_range(start, end, freq=\"D\")\n",
    "\n",
    "    cme_aligned = (\n",
    "        cme_all.set_index(\"date\")\n",
    "        .reindex(full_dates, fill_value=0)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    hp_aligned = (\n",
    "        hp_all.set_index(\"date\")\n",
    "        .reindex(full_dates, fill_value=0)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    merged = pd.merge(cme_aligned, hp_aligned, on=\"date\", how=\"inner\")\n",
    "    return merged\n",
    "\n",
    "# -----------------------------\n",
    "# supervised dataset: lag/rolling + 미래 window max SEP 타깃\n",
    "# -----------------------------\n",
    "def make_supervised_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    base_feature_cols=None,\n",
    "    k_lag: int = 3,\n",
    "    future_window: int = 3,\n",
    "    use_log_target: bool = True\n",
    "):\n",
    "    out = df.sort_values(\"date\").reset_index(drop=True).copy()\n",
    "\n",
    "    if base_feature_cols is None:\n",
    "        base_feature_cols = [c for c in out.columns if c not in [\"date\", \"p_gt_100MeV\"]]\n",
    "\n",
    "    feat_cols = []\n",
    "    for col in base_feature_cols:\n",
    "        for lag in range(1, k_lag + 1):\n",
    "            name = f\"{col}_lag{lag}\"\n",
    "            out[name] = out[col].shift(lag)\n",
    "            feat_cols.append(name)\n",
    "\n",
    "        name = f\"{col}_rollsum{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).sum()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "        name = f\"{col}_rollmax{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).max()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "    # 미래 window max SEP\n",
    "    out[\"target_sep\"] = out[\"p_gt_100MeV\"].shift(-1).rolling(future_window).max()\n",
    "\n",
    "    model_df = out[[\"date\"] + feat_cols + [\"target_sep\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "    X = model_df[feat_cols].values\n",
    "    y = model_df[\"target_sep\"].values\n",
    "\n",
    "    if use_log_target:\n",
    "        y = np.log10(y + 1.0)\n",
    "\n",
    "    return model_df, feat_cols, X, y\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 실행: 1996~2018 데이터셋 생성\n",
    "# -----------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     merged_all = build_merged_all_years_flatdirs(\n",
    "#         1996, 2018,\n",
    "#         cme_data_dir=\"cme_data\",\n",
    "#         hproton_data_dir=\"hproton_data\"\n",
    "#     )\n",
    "\n",
    "#     ds, feature_cols, X, y = make_supervised_dataset(\n",
    "#         merged_all,\n",
    "#         k_lag=3,\n",
    "#         future_window=3,\n",
    "#         use_log_target=True\n",
    "#     )\n",
    "\n",
    "#     print(len(merged_all), len(ds), (ds[\"target_sep\"] > 0).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7a7101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24424\\3687323273.py:279: RuntimeWarning: invalid value encountered in log10\n",
      "  y = np.log10(y + 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto threshold: 1185000.0000000363\n",
      "event rate: 0.005105217283028265\n",
      "Results: {'classifier_auc': np.float64(0.9912718204488778), 'classifier_pr_auc': np.float64(0.46883468834688347), 'regressor_rmse': np.float64(0.11695562440151919), 'regressor_r2': 0.0, 'n_train': 6424, 'n_test': 1607, 'n_train_event': 38, 'n_test_event': 3}\n",
      "Dataset length: 8031\n",
      "Event days: 41\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1) CME 전처리: 월별 txt 모두 로드 -> Halo만 -> 일별 합/횟수 -> 누락일 0 채움\n",
    "# -----------------------------\n",
    "\n",
    "def load_and_preprocess_cme_multifeature(year: int, cme_dir: str):\n",
    "\n",
    "    cme_dir = Path(cme_dir)\n",
    "    files = [cme_dir / f\"univ{year}_{m:02d}.txt\" for m in range(1, 13)]\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        if not f.exists():\n",
    "            continue\n",
    "\n",
    "        # fixed width\n",
    "        df = pd.read_fwf(\n",
    "            f,\n",
    "            skiprows=4,\n",
    "            header=None\n",
    "        )\n",
    "        dfs.append(df)\n",
    "\n",
    "    cme = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # 컬럼 구조 (실제 카탈로그 기준)\n",
    "    # -----------------------------------\n",
    "    # 0 : Date\n",
    "    # 1 : Time\n",
    "    # 2 : Central PA   (or Halo)\n",
    "    # 3 : Width\n",
    "    # 4 : Linear speed\n",
    "    # 5 : 2nd order speed (initial)\n",
    "    # 6 : 2nd order speed (final)\n",
    "    # 7 : 2nd order speed (20R)\n",
    "    # 8 : Accel\n",
    "    # 9 : Mass\n",
    "    # 10: Kinetic energy\n",
    "    # 11: MPA\n",
    "    # 12: Remarks\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    out[\"date\"] = pd.to_datetime(cme.iloc[:, 0], errors=\"coerce\")\n",
    "\n",
    "    # halo / partial halo flag\n",
    "    central = cme.iloc[:, 2].astype(str)\n",
    "    out[\"is_halo\"] = central.str.contains(\"Halo\", case=False, na=False).astype(int)\n",
    "    out[\"is_partial_halo\"] = cme.iloc[:, 12].astype(str).str.contains(\n",
    "        \"Partial\", case=False, na=False\n",
    "    ).astype(int)\n",
    "\n",
    "    # numeric columns\n",
    "    def num(col):\n",
    "        return pd.to_numeric(\n",
    "            cme.iloc[:, col].astype(str).str.replace(\"*\", \"\", regex=False),\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    out[\"width\"] = num(3)\n",
    "    out[\"speed_linear\"] = num(4)\n",
    "    out[\"speed_init\"] = num(5)\n",
    "    out[\"speed_final\"] = num(6)\n",
    "    out[\"speed_20R\"] = num(7)\n",
    "    out[\"accel\"] = num(8)\n",
    "\n",
    "    # mass, energy는 ------- 같은 값이 있으므로 그대로 numeric 처리\n",
    "    out[\"mass\"] = num(9)\n",
    "    out[\"kinetic_energy\"] = num(10)\n",
    "\n",
    "    out = out.dropna(subset=[\"date\"])\n",
    "\n",
    "    # -----------------------------------\n",
    "    # 하루 단위 aggregation\n",
    "    # -----------------------------------\n",
    "    daily = (\n",
    "        out\n",
    "        .groupby(\"date\")\n",
    "        .agg(\n",
    "            cme_count = (\"date\", \"count\"),\n",
    "\n",
    "            halo_count = (\"is_halo\", \"sum\"),\n",
    "            partial_halo_count = (\"is_partial_halo\", \"sum\"),\n",
    "\n",
    "            width_max = (\"width\", \"max\"),\n",
    "            width_mean = (\"width\", \"mean\"),\n",
    "\n",
    "            speed_linear_max = (\"speed_linear\", \"max\"),\n",
    "            speed_linear_mean = (\"speed_linear\", \"mean\"),\n",
    "\n",
    "            speed_init_max = (\"speed_init\", \"max\"),\n",
    "            speed_final_max = (\"speed_final\", \"max\"),\n",
    "            speed_20R_max = (\"speed_20R\", \"max\"),\n",
    "\n",
    "            accel_max = (\"accel\", \"max\"),\n",
    "            accel_min = (\"accel\", \"min\"),\n",
    "\n",
    "            mass_sum = (\"mass\", \"sum\"),\n",
    "            mass_max = (\"mass\", \"max\"),\n",
    "\n",
    "            ke_sum = (\"kinetic_energy\", \"sum\"),\n",
    "            ke_max = (\"kinetic_energy\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # -----------------------------------\n",
    "    # 날짜 채우기\n",
    "    # -----------------------------------\n",
    "    full_dates = pd.date_range(\n",
    "        pd.Timestamp(year, 1, 1),\n",
    "        pd.Timestamp(year, 12, 31),\n",
    "        freq=\"D\"\n",
    "    )\n",
    "\n",
    "    daily = (\n",
    "        daily\n",
    "        .set_index(\"date\")\n",
    "        .reindex(full_dates)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # count 계열은 0, 나머지는 0으로 채워줌(모델용)\n",
    "    count_cols = [\n",
    "        \"cme_count\", \"halo_count\", \"partial_halo_count\"\n",
    "    ]\n",
    "    for c in count_cols:\n",
    "        daily[c] = daily[c].fillna(0)\n",
    "\n",
    "    other_cols = [c for c in daily.columns if c not in [\"date\"] + count_cols]\n",
    "    daily[other_cols] = daily[other_cols].fillna(0)\n",
    "\n",
    "    return daily\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) hproton 전처리: 메타라인 섞여도 안전하게 -> date 만들기 -> p_gt_100만\n",
    "# -----------------------------\n",
    "\n",
    "def load_and_preprocess_hproton(dpd_path: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with open(dpd_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) < 6:   # 최소 year month day p1 p10 p100 있어야 함\n",
    "                continue\n",
    "\n",
    "            # 첫 3개가 정수여야 데이터 줄로 인정\n",
    "            try:\n",
    "                y = int(float(parts[0]))\n",
    "                m = int(float(parts[1]))\n",
    "                d = int(float(parts[2]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            # 데이터 줄이면 앞 9개까지 안전하게 읽기 (없으면 NaN 채움)\n",
    "            vals = parts[:9] + [np.nan] * (9 - len(parts[:9]))\n",
    "            rows.append([y, m, d] + vals[3:9])\n",
    "\n",
    "    hp = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"year\", \"month\", \"day\",\n",
    "            \"p_gt_1MeV\", \"p_gt_10MeV\", \"p_gt_100MeV\",\n",
    "            \"e_gt_0p6MeV\", \"e_gt_2MeV\",\n",
    "            \"neutron_pct\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 숫자 변환\n",
    "    for c in [\"p_gt_1MeV\",\"p_gt_10MeV\",\"p_gt_100MeV\",\"e_gt_0p6MeV\",\"e_gt_2MeV\",\"neutron_pct\"]:\n",
    "        hp[c] = pd.to_numeric(hp[c], errors=\"coerce\")\n",
    "\n",
    "    hp[\"date\"] = pd.to_datetime(hp[[\"year\",\"month\",\"day\"]])\n",
    "    hp = hp[[\"date\", \"p_gt_100MeV\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # 결측은 0으로(원하면 그대로 NaN 둬도 됨)\n",
    "    hp[\"p_gt_100MeV\"] = hp[\"p_gt_100MeV\"].fillna(0.0)\n",
    "\n",
    "    return hp\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) CME + hproton 길이 n 맞추기(동일 date index로 reindex)\n",
    "# -----------------------------\n",
    "def align_daily_series(cme_daily: pd.DataFrame, hp_daily: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 공통 날짜 범위(교집합)로 맞추고 싶으면 여기서 start/end 조절 가능\n",
    "    start = max(cme_daily[\"date\"].min(), hp_daily[\"date\"].min())\n",
    "    end   = min(cme_daily[\"date\"].max(), hp_daily[\"date\"].max())\n",
    "    full_dates = pd.date_range(start, end, freq=\"D\")\n",
    "\n",
    "    cme_aligned = (\n",
    "        cme_daily.set_index(\"date\")\n",
    "        .reindex(full_dates, fill_value=0)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    hp_aligned = (\n",
    "        hp_daily.set_index(\"date\")\n",
    "        .reindex(full_dates, fill_value=0)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    merged = pd.merge(cme_aligned, hp_aligned, on=\"date\", how=\"inner\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) (과거 k일 CME) -> (오늘 SEP) 학습용 테이블 생성\n",
    "# -----------------------------\n",
    "def make_supervised_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    base_feature_cols=None,\n",
    "    k_lag: int = 3,\n",
    "    future_window: int = 3,\n",
    "    use_log_target: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    df: must include columns ['date', 'p_gt_100MeV'] + CME feature columns\n",
    "\n",
    "    base_feature_cols: CME feature columns list.\n",
    "      - None이면 자동으로 (date, p_gt_100MeV) 제외한 모든 컬럼을 feature로 사용.\n",
    "    \"\"\"\n",
    "\n",
    "    out = df.sort_values(\"date\").reset_index(drop=True).copy()\n",
    "\n",
    "    # ✅ feature 컬럼 자동 선택\n",
    "    if base_feature_cols is None:\n",
    "        base_feature_cols = [c for c in out.columns if c not in [\"date\", \"p_gt_100MeV\"]]\n",
    "\n",
    "    # ------------------------\n",
    "    # 1) lag / rolling features\n",
    "    # ------------------------\n",
    "    feat_cols = []\n",
    "    for col in base_feature_cols:\n",
    "        # lag\n",
    "        for lag in range(1, k_lag + 1):\n",
    "            name = f\"{col}_lag{lag}\"\n",
    "            out[name] = out[col].shift(lag)\n",
    "            feat_cols.append(name)\n",
    "\n",
    "        # rolling sum (최근 k일 합)\n",
    "        name = f\"{col}_rollsum{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).sum()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "        # rolling max (최근 k일 최대)\n",
    "        name = f\"{col}_rollmax{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).max()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "    # ------------------------\n",
    "    # 2) target: 미래 window max SEP\n",
    "    # ------------------------\n",
    "    out[\"target_sep\"] = (\n",
    "        out[\"p_gt_100MeV\"]\n",
    "        .shift(-1)\n",
    "        .rolling(future_window)\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # 3) dropna and return\n",
    "    # ------------------------\n",
    "    model_df = out[[\"date\"] + feat_cols + [\"target_sep\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "    X = model_df[feat_cols].values\n",
    "    y = model_df[\"target_sep\"].values\n",
    "\n",
    "    if use_log_target:\n",
    "        y = np.log10(y + 1.0)\n",
    "\n",
    "    return model_df, feat_cols, X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) 모델 학습/평가 (시간 순서 유지)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_two_stage(\n",
    "    ds: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    split_ratio: float = 0.8,\n",
    "    threshold: float = 0.0,      # ✅ event 정의: target_sep > threshold\n",
    "    use_log_intensity: bool = True,\n",
    "    random_state: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    ds: make_supervised_dataset()가 반환한 model_df (date, feature_cols..., target_sep 포함)\n",
    "    feature_cols: feature column names\n",
    "    threshold: event 기준(>100MeV pfu를 어떤 값 이상으로 event로 볼지)\n",
    "               - 너가 '0보다 크면 발생'으로 갈 거면 0.0\n",
    "               - NOAA식 임계값을 쓰고 싶으면 예: 1.0 또는 10.0 등으로 바꿔도 됨\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 0) Train/Test time split\n",
    "    # -------------------------\n",
    "    ds = ds.sort_values(\"date\").reset_index(drop=True)\n",
    "    n = len(ds)\n",
    "    split = int(n * split_ratio)\n",
    "\n",
    "    train_df = ds.iloc[:split].copy()\n",
    "    test_df  = ds.iloc[split:].copy()\n",
    "\n",
    "    X_train = train_df[feature_cols].values\n",
    "    X_test  = test_df[feature_cols].values\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Stage-1: Event classifier\n",
    "    # -------------------------\n",
    "    y_train_cls = (train_df[\"target_sep\"].values > threshold).astype(int)\n",
    "    y_test_cls  = (test_df[\"target_sep\"].values > threshold).astype(int)\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    clf.fit(X_train, y_train_cls)\n",
    "\n",
    "    p_test = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # 분류 성능(희소 이벤트면 PR-AUC가 더 중요)\n",
    "    auc = roc_auc_score(y_test_cls, p_test) if len(np.unique(y_test_cls)) > 1 else np.nan\n",
    "    pr_auc = average_precision_score(y_test_cls, p_test) if len(np.unique(y_test_cls)) > 1 else np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Stage-2: Intensity regressor (event인 샘플만)\n",
    "    # -------------------------\n",
    "    train_event = train_df[train_df[\"target_sep\"] > threshold].copy()\n",
    "    test_event  = test_df[test_df[\"target_sep\"] > threshold].copy()\n",
    "\n",
    "    # event가 너무 적으면 회귀 자체가 불안정할 수 있음\n",
    "    if len(train_event) < 10:\n",
    "        raise ValueError(f\"Too few event samples in train set: {len(train_event)}. \"\n",
    "                         f\"Try lowering threshold or using more years.\")\n",
    "\n",
    "    X_train_reg = train_event[feature_cols].values\n",
    "    X_test_reg  = test_event[feature_cols].values\n",
    "\n",
    "    y_train_reg_raw = train_event[\"target_sep\"].values\n",
    "    y_test_reg_raw  = test_event[\"target_sep\"].values\n",
    "\n",
    "    if use_log_intensity:\n",
    "        y_train_reg = np.log10(y_train_reg_raw + 1.0)\n",
    "        y_test_reg  = np.log10(y_test_reg_raw + 1.0)\n",
    "    else:\n",
    "        y_train_reg = y_train_reg_raw\n",
    "        y_test_reg  = y_test_reg_raw\n",
    "\n",
    "    reg = RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        max_depth=10,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "    # test의 \"진짜 event\"들에 대해서만 intensity 회귀 평가\n",
    "    pred_reg = reg.predict(X_test_reg)\n",
    "\n",
    "    mse = mean_squared_error(y_test_reg, pred_reg)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_reg, pred_reg) if len(y_test_reg) >= 2 else np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Two-stage 합성 예측(원하면 활용)\n",
    "    # -------------------------\n",
    "    # test 전체 날짜에 대해:\n",
    "    #   - event 확률 p_test\n",
    "    #   - intensity는 \"예측 event\"에만 계산해도 되지만,\n",
    "    #     기대값 형태로 쓰려면 전체에 대해 reg를 돌려도 됨\n",
    "    pred_reg_all = reg.predict(X_test)  # log 스케일일 수도 있음\n",
    "\n",
    "    if use_log_intensity:\n",
    "        intensity_all = np.maximum(0.0, 10**pred_reg_all - 1.0)\n",
    "    else:\n",
    "        intensity_all = np.maximum(0.0, pred_reg_all)\n",
    "\n",
    "    expected_sep = p_test * intensity_all\n",
    "\n",
    "    results = {\n",
    "        \"classifier_auc\": auc,\n",
    "        \"classifier_pr_auc\": pr_auc,\n",
    "        \"regressor_rmse\": rmse,    # (log 스케일이면 log 기준 RMSE)\n",
    "        \"regressor_r2\": r2,\n",
    "        \"n_train\": len(train_df),\n",
    "        \"n_test\": len(test_df),\n",
    "        \"n_train_event\": len(train_event),\n",
    "        \"n_test_event\": len(test_event),\n",
    "    }\n",
    "\n",
    "    # test 결과 dataframe(원하면 저장/플롯)\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"date\": test_df[\"date\"].values,\n",
    "        \"target_sep\": test_df[\"target_sep\"].values,\n",
    "        \"event_true\": y_test_cls,\n",
    "        \"p_event\": p_test,\n",
    "        \"intensity_pred\": intensity_all,\n",
    "        \"expected_sep\": expected_sep\n",
    "    })\n",
    "\n",
    "    return clf, reg, results, pred_df\n",
    "\n",
    "# -----------------------------\n",
    "# 실행 예시\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    merged = build_merged_all_years(\n",
    "        start_year=1996,\n",
    "        end_year=2018,\n",
    "        cme_dir=\"./cme_data\",\n",
    "        hproton_dir=\"./hproton_data\"\n",
    "    )\n",
    "\n",
    "    ds, feature_cols, X, y = make_supervised_dataset(\n",
    "        merged,\n",
    "        k_lag=3,\n",
    "        future_window=3,\n",
    "        use_log_target=True\n",
    "    )\n",
    "\n",
    "    thr = ds[\"target_sep\"].quantile(0.995)\n",
    "    print(\"auto threshold:\", thr)\n",
    "    print(\"event rate:\", (ds[\"target_sep\"] > thr).mean())\n",
    "\n",
    "\n",
    "    # ✅ 2-stage 실행\n",
    "    clf, reg, results, pred_df = train_and_evaluate_two_stage(\n",
    "        ds,\n",
    "        feature_cols,\n",
    "        threshold=thr\n",
    "    )\n",
    "\n",
    "    print(\"Results:\", results)\n",
    "    print(\"Dataset length:\", len(ds))\n",
    "    print(\"Event days:\", (ds[\"target_sep\"] > thr).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9660259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_sep > 0 : 0.9905366704021915\n",
      "quantiles: 0.000   -1.000000e+05\n",
      "0.500    3.500000e+03\n",
      "0.900    4.700000e+03\n",
      "0.950    6.500000e+03\n",
      "0.990    2.800000e+05\n",
      "0.999    6.073000e+06\n",
      "Name: target_sep, dtype: float64\n",
      "top10: [13000000.0, 13000000.0, 13000000.0, 11000000.0, 11000000.0, 11000000.0, 6100000.0, 6100000.0, 6100000.0, 5200000.0]\n"
     ]
    }
   ],
   "source": [
    "s = ds[\"target_sep\"]\n",
    "\n",
    "print(\"target_sep > 0 :\", (s > 0).mean())\n",
    "print(\"quantiles:\", s.quantile([0, 0.5, 0.9, 0.95, 0.99, 0.999]))\n",
    "print(\"top10:\", s.sort_values(ascending=False).head(10).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44d948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr=      0  event_rate=0.9905  events=7955\n",
      "thr= 0.0001  event_rate=0.9905  events=7955\n",
      "thr=  0.001  event_rate=0.9905  events=7955\n",
      "thr=   0.01  event_rate=0.9905  events=7955\n",
      "thr=   0.05  event_rate=0.9905  events=7955\n",
      "thr=    0.1  event_rate=0.9905  events=7955\n",
      "thr=    0.5  event_rate=0.9905  events=7955\n",
      "thr=    1.0  event_rate=0.9905  events=7955\n",
      "thr=    5.0  event_rate=0.9905  events=7955\n",
      "thr=   10.0  event_rate=0.9905  events=7955\n"
     ]
    }
   ],
   "source": [
    "def summarize_event_rates(ds):\n",
    "    for thr in [0, 1e-4, 1e-3, 1e-2, 5e-2, 0.1, 0.5, 1.0, 5.0, 10.0]:\n",
    "        rate = (ds[\"target_sep\"] > thr).mean()\n",
    "        print(f\"thr={thr:>7}  event_rate={rate:.4f}  events={int((ds['target_sep']>thr).sum())}\")\n",
    "\n",
    "summarize_event_rates(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d038b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
