{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0d689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f53eb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 1185000.0000000363\n",
      "event_rate: 0.005105217283028265\n",
      "results: {'classifier_auc': np.float64(0.9912718204488778), 'classifier_pr_auc': np.float64(0.46883468834688347), 'regressor_rmse': 0.1169556244015193, 'regressor_r2': 0.0, 'n_train': 6424, 'n_test': 1607, 'n_train_event': 38, 'n_test_event': 3}\n",
      "test_date: 2014-08-07\n",
      "prediction: {'asof_date': Timestamp('2014-08-07 00:00:00'), 'p_event': 0.007710076687263456, 'intensity_if_event': 2990336.304347603, 'expected_intensity': 23055.722227228012}\n",
      "true_target_sep: 2600.0\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_cme_year(year: int, cme_data_dir: str) -> pd.DataFrame:\n",
    "    cme_data_dir = Path(cme_data_dir)\n",
    "    files = [cme_data_dir / f\"univ{year}_{m:02d}.txt\" for m in range(1, 13)]\n",
    "    full_dates = pd.date_range(pd.Timestamp(year, 1, 1), pd.Timestamp(year, 12, 31), freq=\"D\")\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        if f.exists():\n",
    "            dfs.append(pd.read_fwf(f, skiprows=4, header=None))\n",
    "\n",
    "    if not dfs:\n",
    "        daily = pd.DataFrame({\"date\": full_dates})\n",
    "        cols = [\n",
    "            \"cme_count\",\"halo_count\",\"partial_halo_count\",\n",
    "            \"width_max\",\"width_mean\",\n",
    "            \"speed_linear_max\",\"speed_linear_mean\",\n",
    "            \"speed_init_max\",\"speed_final_max\",\"speed_20R_max\",\n",
    "            \"accel_max\",\"accel_min\",\n",
    "            \"mass_sum\",\"mass_max\",\n",
    "            \"ke_sum\",\"ke_max\"\n",
    "        ]\n",
    "        for c in cols:\n",
    "            daily[c] = 0.0\n",
    "        return daily\n",
    "\n",
    "    cme = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if cme.shape[1] < 13:\n",
    "        for _ in range(13 - cme.shape[1]):\n",
    "            cme[cme.shape[1]] = np.nan\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"date\"] = pd.to_datetime(cme.iloc[:, 0], errors=\"coerce\")\n",
    "\n",
    "    central = cme.iloc[:, 2].astype(str)\n",
    "    out[\"is_halo\"] = central.str.contains(\"Halo\", case=False, na=False).astype(int)\n",
    "    out[\"is_partial_halo\"] = cme.iloc[:, 12].astype(str).str.contains(\"Partial\", case=False, na=False).astype(int)\n",
    "\n",
    "    def num(col):\n",
    "        return pd.to_numeric(\n",
    "            cme.iloc[:, col].astype(str).str.replace(\"*\", \"\", regex=False),\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    out[\"width\"] = num(3)\n",
    "    out[\"speed_linear\"] = num(4)\n",
    "    out[\"speed_init\"] = num(5)\n",
    "    out[\"speed_final\"] = num(6)\n",
    "    out[\"speed_20R\"] = num(7)\n",
    "    out[\"accel\"] = num(8)\n",
    "    out[\"mass\"] = num(9)\n",
    "    out[\"kinetic_energy\"] = num(10)\n",
    "\n",
    "    out = out.dropna(subset=[\"date\"])\n",
    "\n",
    "    daily = (\n",
    "        out.groupby(\"date\")\n",
    "        .agg(\n",
    "            cme_count=(\"date\", \"count\"),\n",
    "            halo_count=(\"is_halo\", \"sum\"),\n",
    "            partial_halo_count=(\"is_partial_halo\", \"sum\"),\n",
    "            width_max=(\"width\", \"max\"),\n",
    "            width_mean=(\"width\", \"mean\"),\n",
    "            speed_linear_max=(\"speed_linear\", \"max\"),\n",
    "            speed_linear_mean=(\"speed_linear\", \"mean\"),\n",
    "            speed_init_max=(\"speed_init\", \"max\"),\n",
    "            speed_final_max=(\"speed_final\", \"max\"),\n",
    "            speed_20R_max=(\"speed_20R\", \"max\"),\n",
    "            accel_max=(\"accel\", \"max\"),\n",
    "            accel_min=(\"accel\", \"min\"),\n",
    "            mass_sum=(\"mass\", \"sum\"),\n",
    "            mass_max=(\"mass\", \"max\"),\n",
    "            ke_sum=(\"kinetic_energy\", \"sum\"),\n",
    "            ke_max=(\"kinetic_energy\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    daily = (\n",
    "        daily.set_index(\"date\")\n",
    "        .reindex(full_dates)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    return daily\n",
    "\n",
    "\n",
    "def load_and_preprocess_hproton_year(year: int, hproton_data_dir: str) -> pd.DataFrame:\n",
    "    dpd_path = Path(hproton_data_dir) / f\"{year}_DPD.txt\"\n",
    "    if not dpd_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing DPD file: {dpd_path}\")\n",
    "\n",
    "    rows = []\n",
    "    with open(dpd_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 6:\n",
    "                continue\n",
    "            try:\n",
    "                y = int(float(parts[0])); m = int(float(parts[1])); d = int(float(parts[2]))\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            vals = parts[:9] + [np.nan] * (9 - len(parts[:9]))\n",
    "            rows.append([y, m, d] + vals[3:9])\n",
    "\n",
    "    hp = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"year\",\"month\",\"day\",\n",
    "            \"p_gt_1MeV\",\"p_gt_10MeV\",\"p_gt_100MeV\",\n",
    "            \"e_gt_0p6MeV\",\"e_gt_2MeV\",\n",
    "            \"neutron_pct\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hp[\"p_gt_100MeV\"] = pd.to_numeric(hp[\"p_gt_100MeV\"], errors=\"coerce\")\n",
    "    hp[\"p_gt_100MeV\"] = hp[\"p_gt_100MeV\"].where(hp[\"p_gt_100MeV\"] >= 0)\n",
    "    hp[\"p_gt_100MeV\"] = hp[\"p_gt_100MeV\"].fillna(0.0).astype(float)\n",
    "    hp[\"date\"] = pd.to_datetime(hp[[\"year\",\"month\",\"day\"]])\n",
    "\n",
    "    daily = hp[[\"date\",\"p_gt_100MeV\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "    return daily\n",
    "\n",
    "\n",
    "def build_merged_all_years(start_year: int, end_year: int, cme_dir: str, hproton_dir: str) -> pd.DataFrame:\n",
    "    cme_all = []\n",
    "    hp_all = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        cme_all.append(load_and_preprocess_cme_year(year, cme_dir))\n",
    "\n",
    "        try:\n",
    "            hp_all.append(load_and_preprocess_hproton_year(year, hproton_dir))\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    cme_all = pd.concat(cme_all, ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "    hp_all = pd.concat(hp_all, ignore_index=True).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    start = max(cme_all[\"date\"].min(), hp_all[\"date\"].min())\n",
    "    end = min(cme_all[\"date\"].max(), hp_all[\"date\"].max())\n",
    "    full_dates = pd.date_range(start, end, freq=\"D\")\n",
    "\n",
    "    cme_aligned = (\n",
    "        cme_all.set_index(\"date\")\n",
    "        .reindex(full_dates, fill_value=0.0)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    hp_aligned = (\n",
    "        hp_all.set_index(\"date\")\n",
    "        .reindex(full_dates, fill_value=0.0)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    merged = pd.merge(cme_aligned, hp_aligned, on=\"date\", how=\"inner\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def make_supervised_dataset(df: pd.DataFrame, k_lag: int = 3, future_window: int = 3):\n",
    "    out = df.sort_values(\"date\").reset_index(drop=True).copy()\n",
    "    base_cols = [c for c in out.columns if c not in [\"date\", \"p_gt_100MeV\"]]\n",
    "\n",
    "    feat_cols = []\n",
    "    for col in base_cols:\n",
    "        for lag in range(1, k_lag + 1):\n",
    "            name = f\"{col}_lag{lag}\"\n",
    "            out[name] = out[col].shift(lag)\n",
    "            feat_cols.append(name)\n",
    "\n",
    "        name = f\"{col}_rollsum{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).sum()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "        name = f\"{col}_rollmax{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).max()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "    out[\"target_sep\"] = out[\"p_gt_100MeV\"].shift(-1).rolling(future_window).max()\n",
    "\n",
    "    ds = out[[\"date\"] + feat_cols + [\"target_sep\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "    return ds, feat_cols, base_cols\n",
    "\n",
    "\n",
    "def train_two_stage(ds: pd.DataFrame, feature_cols: list, threshold: float, split_ratio: float = 0.8, random_state: int = 0):\n",
    "    ds = ds.sort_values(\"date\").reset_index(drop=True)\n",
    "    n = len(ds)\n",
    "    split = int(n * split_ratio)\n",
    "\n",
    "    train_df = ds.iloc[:split].copy()\n",
    "    test_df = ds.iloc[split:].copy()\n",
    "\n",
    "    X_train = train_df[feature_cols].values\n",
    "    X_test = test_df[feature_cols].values\n",
    "\n",
    "    y_train_cls = (train_df[\"target_sep\"].values > threshold).astype(int)\n",
    "    y_test_cls = (test_df[\"target_sep\"].values > threshold).astype(int)\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=500, max_depth=8,\n",
    "        random_state=random_state, n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    clf.fit(X_train, y_train_cls)\n",
    "\n",
    "    if 1 in clf.classes_:\n",
    "        idx1 = int(np.where(clf.classes_ == 1)[0][0])\n",
    "        p_test = clf.predict_proba(X_test)[:, idx1]\n",
    "    else:\n",
    "        p_test = np.zeros(len(X_test), dtype=float)\n",
    "\n",
    "    auc = roc_auc_score(y_test_cls, p_test) if len(np.unique(y_test_cls)) > 1 else np.nan\n",
    "    pr_auc = average_precision_score(y_test_cls, p_test) if len(np.unique(y_test_cls)) > 1 else np.nan\n",
    "\n",
    "    train_event = train_df[train_df[\"target_sep\"] > threshold].copy()\n",
    "    test_event = test_df[test_df[\"target_sep\"] > threshold].copy()\n",
    "\n",
    "    if len(train_event) < 10:\n",
    "        raise ValueError(f\"Too few train events: {len(train_event)} (threshold={threshold})\")\n",
    "\n",
    "    X_train_reg = train_event[feature_cols].values\n",
    "    y_train_reg = np.log10(train_event[\"target_sep\"].values + 1.0)\n",
    "\n",
    "    reg = RandomForestRegressor(\n",
    "        n_estimators=800, max_depth=10,\n",
    "        random_state=random_state, n_jobs=-1\n",
    "    )\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "    if len(test_event) > 0:\n",
    "        X_test_reg = test_event[feature_cols].values\n",
    "        y_test_reg = np.log10(test_event[\"target_sep\"].values + 1.0)\n",
    "        pred_reg = reg.predict(X_test_reg)\n",
    "        rmse = float(np.sqrt(mean_squared_error(y_test_reg, pred_reg)))\n",
    "        r2 = float(r2_score(y_test_reg, pred_reg)) if len(y_test_reg) >= 2 else np.nan\n",
    "    else:\n",
    "        rmse, r2 = np.nan, np.nan\n",
    "\n",
    "    pred_reg_all = reg.predict(X_test)\n",
    "    intensity_all = np.maximum(0.0, 10**pred_reg_all - 1.0)\n",
    "    expected_sep = p_test * intensity_all\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"date\": test_df[\"date\"].values,\n",
    "        \"target_sep\": test_df[\"target_sep\"].values,\n",
    "        \"event_true\": y_test_cls,\n",
    "        \"p_event\": p_test,\n",
    "        \"intensity_pred\": intensity_all,\n",
    "        \"expected_sep\": expected_sep\n",
    "    })\n",
    "\n",
    "    results = {\n",
    "        \"classifier_auc\": auc,\n",
    "        \"classifier_pr_auc\": pr_auc,\n",
    "        \"regressor_rmse\": rmse,\n",
    "        \"regressor_r2\": r2,\n",
    "        \"n_train\": len(train_df),\n",
    "        \"n_test\": len(test_df),\n",
    "        \"n_train_event\": int((train_df[\"target_sep\"] > threshold).sum()),\n",
    "        \"n_test_event\": int((test_df[\"target_sep\"] > threshold).sum()),\n",
    "    }\n",
    "\n",
    "    return clf, reg, results, pred_df\n",
    "\n",
    "\n",
    "def save_model(path: str, clf, reg, meta: dict):\n",
    "    joblib.dump({\"clf\": clf, \"reg\": reg, \"meta\": meta}, path)\n",
    "\n",
    "\n",
    "def load_model(path: str):\n",
    "    pkg = joblib.load(path)\n",
    "    return pkg[\"clf\"], pkg[\"reg\"], pkg[\"meta\"]\n",
    "\n",
    "\n",
    "def build_feature_row_asof(merged: pd.DataFrame, base_cols: list, feature_cols: list, k_lag: int, asof_date: str):\n",
    "    df = merged[[\"date\"] + base_cols].sort_values(\"date\").reset_index(drop=True).copy()\n",
    "    asof_date = pd.to_datetime(asof_date)\n",
    "\n",
    "    for col in base_cols:\n",
    "        for lag in range(1, k_lag + 1):\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "        df[f\"{col}_rollsum{k_lag}\"] = df[col].shift(1).rolling(k_lag).sum()\n",
    "        df[f\"{col}_rollmax{k_lag}\"] = df[col].shift(1).rolling(k_lag).max()\n",
    "\n",
    "    row = df[df[\"date\"] == asof_date]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"asof_date not found in merged date range: {asof_date.date()}\")\n",
    "\n",
    "    row = row[[\"date\"] + feature_cols].copy()\n",
    "    if row.isna().any(axis=None):\n",
    "        raise ValueError(\"Not enough history before asof_date to build lag/rolling features.\")\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def predict_two_stage(clf, reg, meta: dict, feature_row: pd.DataFrame):\n",
    "    feature_cols = meta[\"feature_cols\"]\n",
    "    X = feature_row[feature_cols].values\n",
    "\n",
    "    if 1 in clf.classes_:\n",
    "        idx1 = int(np.where(clf.classes_ == 1)[0][0])\n",
    "        p_event = float(clf.predict_proba(X)[:, idx1][0])\n",
    "    else:\n",
    "        p_event = 0.0\n",
    "\n",
    "    pred_log_int = float(reg.predict(X)[0])\n",
    "    intensity = max(0.0, 10**pred_log_int - 1.0)\n",
    "    expected = p_event * intensity\n",
    "\n",
    "    return {\n",
    "        \"asof_date\": pd.to_datetime(feature_row[\"date\"].iloc[0]),\n",
    "        \"p_event\": p_event,\n",
    "        \"intensity_if_event\": intensity,\n",
    "        \"expected_intensity\": expected\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cme_dir = \"./cme_data\"\n",
    "    hproton_dir = \"./hproton_data\"\n",
    "    start_year, end_year = 1996, 2018\n",
    "    k_lag, future_window = 3, 3\n",
    "\n",
    "    merged = build_merged_all_years(start_year, end_year, cme_dir, hproton_dir)\n",
    "    ds, feature_cols, base_cols = make_supervised_dataset(merged, k_lag=k_lag, future_window=future_window)\n",
    "\n",
    "    thr = float(ds[\"target_sep\"].quantile(0.995))\n",
    "\n",
    "    clf, reg, results, pred_df = train_two_stage(ds, feature_cols, threshold=thr)\n",
    "\n",
    "    print(\"threshold:\", thr)\n",
    "    print(\"event_rate:\", float((ds[\"target_sep\"] > thr).mean()))\n",
    "    print(\"results:\", results)\n",
    "\n",
    "    meta = {\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"base_cols\": base_cols,\n",
    "        \"k_lag\": k_lag,\n",
    "        \"future_window\": future_window,\n",
    "        \"threshold\": thr,\n",
    "        \"use_log_intensity\": True,\n",
    "        \"start_year\": start_year,\n",
    "        \"end_year\": end_year\n",
    "    }\n",
    "\n",
    "    model_path = \"sep_two_stage_1996_2018.joblib\"\n",
    "    save_model(model_path, clf, reg, meta)\n",
    "\n",
    "    clf2, reg2, meta2 = load_model(model_path)\n",
    "\n",
    "    test_date = str(pred_df[\"date\"].iloc[0].date())\n",
    "    feature_row = build_feature_row_asof(merged, meta2[\"base_cols\"], meta2[\"feature_cols\"], meta2[\"k_lag\"], test_date)\n",
    "    pred = predict_two_stage(clf2, reg2, meta2, feature_row)\n",
    "\n",
    "    print(\"test_date:\", test_date)\n",
    "    print(\"prediction:\", pred)\n",
    "    print(\"true_target_sep:\", float(ds.loc[ds[\"date\"] == pd.to_datetime(test_date), \"target_sep\"].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f4f7b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date   p_event  intensity_pred  expected_sep\n",
      "108 2002-04-22  0.758742    2.056767e+06  1.560555e+06\n",
      "107 2002-04-21  0.633779    1.802825e+06  1.142593e+06\n",
      "106 2002-04-20  0.628227    2.441486e+06  1.533807e+06\n",
      "110 2002-04-24  0.621117    3.576100e+06  2.221176e+06\n",
      "109 2002-04-23  0.575246    3.725235e+06  2.142927e+06\n",
      "105 2002-04-19  0.350141    3.805157e+06  1.332340e+06\n",
      "104 2002-04-18  0.191780    3.023989e+06  5.799405e+05\n",
      "116 2002-04-30  0.100704    2.488224e+06  2.505743e+05\n",
      "115 2002-04-29  0.073505    2.805780e+06  2.062376e+05\n",
      "117 2002-05-01  0.068787    5.024932e+06  3.456483e+05\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CME txt(월별/단일파일) -> 일별 집계(daily)로 변환\n",
    "# -----------------------------\n",
    "def load_cme_txt_to_daily(cme_txt_path: Union[str, Path], year: int) -> pd.DataFrame:\n",
    "    cme_txt_path = Path(cme_txt_path)\n",
    "\n",
    "    # fixed width\n",
    "    df = pd.read_fwf(cme_txt_path, skiprows=4, header=None)\n",
    "\n",
    "    # 열 부족 방어(포맷 변형)\n",
    "    if df.shape[1] < 13:\n",
    "        for _ in range(13 - df.shape[1]):\n",
    "            df[df.shape[1]] = np.nan\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"date\"] = pd.to_datetime(df.iloc[:, 0], errors=\"coerce\")\n",
    "\n",
    "    central = df.iloc[:, 2].astype(str)\n",
    "    out[\"is_halo\"] = central.str.contains(\"Halo\", case=False, na=False).astype(int)\n",
    "    out[\"is_partial_halo\"] = df.iloc[:, 12].astype(str).str.contains(\"Partial\", case=False, na=False).astype(int)\n",
    "\n",
    "    def num(col: int):\n",
    "        return pd.to_numeric(\n",
    "            df.iloc[:, col].astype(str).str.replace(\"*\", \"\", regex=False),\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    out[\"width\"] = num(3)\n",
    "    out[\"speed_linear\"] = num(4)\n",
    "    out[\"speed_init\"] = num(5)\n",
    "    out[\"speed_final\"] = num(6)\n",
    "    out[\"speed_20R\"] = num(7)\n",
    "    out[\"accel\"] = num(8)\n",
    "    out[\"mass\"] = num(9)\n",
    "    out[\"kinetic_energy\"] = num(10)\n",
    "\n",
    "    out = out.dropna(subset=[\"date\"])\n",
    "\n",
    "    daily = (\n",
    "        out.groupby(\"date\")\n",
    "        .agg(\n",
    "            cme_count=(\"date\", \"count\"),\n",
    "            halo_count=(\"is_halo\", \"sum\"),\n",
    "            partial_halo_count=(\"is_partial_halo\", \"sum\"),\n",
    "\n",
    "            width_max=(\"width\", \"max\"),\n",
    "            width_mean=(\"width\", \"mean\"),\n",
    "\n",
    "            speed_linear_max=(\"speed_linear\", \"max\"),\n",
    "            speed_linear_mean=(\"speed_linear\", \"mean\"),\n",
    "\n",
    "            speed_init_max=(\"speed_init\", \"max\"),\n",
    "            speed_final_max=(\"speed_final\", \"max\"),\n",
    "            speed_20R_max=(\"speed_20R\", \"max\"),\n",
    "\n",
    "            accel_max=(\"accel\", \"max\"),\n",
    "            accel_min=(\"accel\", \"min\"),\n",
    "\n",
    "            mass_sum=(\"mass\", \"sum\"),\n",
    "            mass_max=(\"mass\", \"max\"),\n",
    "\n",
    "            ke_sum=(\"kinetic_energy\", \"sum\"),\n",
    "            ke_max=(\"kinetic_energy\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # 예측할 때도 \"연도 전체 날짜축\"으로 맞춰주는 게 편함\n",
    "    full_dates = pd.date_range(pd.Timestamp(year, 1, 1), pd.Timestamp(year, 12, 31), freq=\"D\")\n",
    "    daily = (\n",
    "        daily.set_index(\"date\")\n",
    "        .reindex(full_dates)\n",
    "        .rename_axis(\"date\")\n",
    "        .reset_index()\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    return daily\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# daily CME -> (학습과 동일한 방식) lag/rolling feature 생성 (target 없음)\n",
    "# -----------------------------\n",
    "def make_features_only(\n",
    "    daily_cme: pd.DataFrame,\n",
    "    base_feature_cols: Optional[List[str]] = None,\n",
    "    k_lag: int = 3,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    out = daily_cme.sort_values(\"date\").reset_index(drop=True).copy()\n",
    "\n",
    "    if base_feature_cols is None:\n",
    "        base_feature_cols = [c for c in out.columns if c != \"date\"]\n",
    "\n",
    "    feat_cols: List[str] = []\n",
    "    for col in base_feature_cols:\n",
    "        for lag in range(1, k_lag + 1):\n",
    "            name = f\"{col}_lag{lag}\"\n",
    "            out[name] = out[col].shift(lag)\n",
    "            feat_cols.append(name)\n",
    "\n",
    "        name = f\"{col}_rollsum{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).sum()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "        name = f\"{col}_rollmax{k_lag}\"\n",
    "        out[name] = out[col].shift(1).rolling(k_lag).max()\n",
    "        feat_cols.append(name)\n",
    "\n",
    "    feat_df = out[[\"date\"] + feat_cols].dropna().reset_index(drop=True)\n",
    "    return feat_df, feat_cols\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 모델 저장/불러오기\n",
    "# -----------------------------\n",
    "def save_two_stage_model(\n",
    "    path: Union[str, Path],\n",
    "    clf: RandomForestClassifier,\n",
    "    reg: RandomForestRegressor,\n",
    "    feature_cols: List[str],\n",
    "    *,\n",
    "    threshold: float,\n",
    "    k_lag: int,\n",
    "    use_log_intensity: bool = True,\n",
    ") -> None:\n",
    "    payload = {\n",
    "        \"clf\": clf,\n",
    "        \"reg\": reg,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"threshold\": float(threshold),\n",
    "        \"k_lag\": int(k_lag),\n",
    "        \"use_log_intensity\": bool(use_log_intensity),\n",
    "    }\n",
    "    joblib.dump(payload, Path(path))\n",
    "\n",
    "\n",
    "def load_two_stage_model(path: Union[str, Path]) -> Dict:\n",
    "    return joblib.load(Path(path))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 저장된 모델로 예측\n",
    "#   - p_event: \"미래 window max SEP > threshold\"일 확률(위험도)\n",
    "#   - intensity_pred: event일 때 강도 예측(학습과 동일한 log 스케일 변환을 되돌림)\n",
    "#   - expected_sep = p_event * intensity_pred\n",
    "# -----------------------------\n",
    "def predict_sep_from_cme_daily(\n",
    "    model_bundle: Dict,\n",
    "    daily_cme: pd.DataFrame,\n",
    "    *,\n",
    "    base_feature_cols: Optional[List[str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    clf: RandomForestClassifier = model_bundle[\"clf\"]\n",
    "    reg: RandomForestRegressor = model_bundle[\"reg\"]\n",
    "\n",
    "    feature_cols: List[str] = model_bundle[\"meta\"][\"feature_cols\"]\n",
    "    k_lag: int = model_bundle[\"meta\"][\"k_lag\"]\n",
    "    use_log_intensity: bool = model_bundle[\"meta\"][\"use_log_intensity\"]\n",
    "\n",
    "    feat_df, _ = make_features_only(daily_cme, base_feature_cols=base_feature_cols, k_lag=k_lag)\n",
    "\n",
    "    # 학습 당시 feature_cols 순서/이름과 반드시 일치해야 함\n",
    "    X = feat_df[feature_cols].values\n",
    "\n",
    "    proba = clf.predict_proba(X)\n",
    "    # 어떤 경우(학습 데이터에 class가 1종류만 있는 경우) shape이 (n,1)일 수 있어 방어\n",
    "    if proba.shape[1] == 2:\n",
    "        p_event = proba[:, 1]\n",
    "    else:\n",
    "        # class가 1개뿐이면 \"항상 그 class\"라고 보면 됨\n",
    "        only_class = int(getattr(clf, \"classes_\", [0])[0])\n",
    "        p_event = np.ones(len(X)) if only_class == 1 else np.zeros(len(X))\n",
    "\n",
    "    pred_reg = reg.predict(X)\n",
    "    if use_log_intensity:\n",
    "        intensity = np.maximum(0.0, (10 ** pred_reg) - 1.0)\n",
    "    else:\n",
    "        intensity = np.maximum(0.0, pred_reg)\n",
    "\n",
    "    expected_sep = p_event * intensity\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"date\": feat_df[\"date\"].values,\n",
    "        \"p_event\": p_event,\n",
    "        \"intensity_pred\": intensity,\n",
    "        \"expected_sep\": expected_sep,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# (예시) \"이런 CME txt 1개\"로 바로 테스트\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # (A) 학습이 끝난 뒤 저장할 때:\n",
    "    # save_two_stage_model(\n",
    "    #     \"sep_two_stage.joblib\",\n",
    "    #     clf, reg, feature_cols,\n",
    "    #     threshold=thr, k_lag=3,\n",
    "    #     use_log_intensity=True\n",
    "    # )\n",
    "\n",
    "    # (B) 저장된 모델 불러오기\n",
    "    bundle = load_two_stage_model(\"sep_two_stage_1996_2018.joblib\")\n",
    "\n",
    "    # (C) 지금 받은 CME txt(예: 업로드된 파일) -> daily -> 예측\n",
    "    cme_txt = \"./cme_data/univ2002_04.txt\"\n",
    "    daily = load_cme_txt_to_daily(cme_txt, year=2002)\n",
    "\n",
    "    pred = predict_sep_from_cme_daily(bundle, daily)\n",
    "\n",
    "    # 상위 위험일(확률 높은 날) 확인\n",
    "    print(pred.sort_values(\"p_event\", ascending=False).head(10))\n",
    "\n",
    "    # 특정 날짜만 보고 싶으면:\n",
    "    # print(pred[pred[\"date\"].between(\"2002-03-10\", \"2002-03-20\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b944cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiple_cme_txts(\n",
    "    txt_list,\n",
    "    year\n",
    "):\n",
    "    \n",
    "    # clf, reg, meta = bundle\n",
    "    # 1) 각 txt → daily\n",
    "    daily_list = []\n",
    "    for p in txt_list:\n",
    "        d = load_cme_txt_to_daily(p, year=year)\n",
    "        daily_list.append(d)\n",
    "\n",
    "    # 2) 날짜 기준으로 합치기\n",
    "    daily_all = (\n",
    "        pd.concat(daily_list, ignore_index=True)\n",
    "        .groupby(\"date\", as_index=False)\n",
    "        .sum()\n",
    "        .sort_values(\"date\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return daily_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccaaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   p_event  intensity_pred  expected_sep\n",
      "0 2000-01-04  0.001787    3.175243e+06   5672.771331\n",
      "1 2000-01-05  0.003502    3.702144e+06  12966.091915\n",
      "2 2000-01-06  0.000000    4.413691e+06      0.000000\n",
      "3 2000-01-07  0.005675    6.517853e+06  36987.197899\n",
      "4 2000-01-08  0.002000    3.116318e+06   6232.636113\n",
      "          date   p_event  intensity_pred  expected_sep\n",
      "193 2000-07-15  0.867414    7.660187e+06  6.644554e+06\n",
      "194 2000-07-16  0.817765    4.411257e+06  3.607371e+06\n",
      "191 2000-07-13  0.754831    8.360004e+06  6.310386e+06\n",
      "311 2000-11-10  0.753068    8.102358e+06  6.101623e+06\n",
      "192 2000-07-14  0.728511    6.803753e+06  4.956611e+06\n",
      "130 2000-05-13  0.728029    4.383728e+06  3.191481e+06\n",
      "131 2000-05-14  0.692193    5.799346e+06  4.014264e+06\n",
      "310 2000-11-09  0.666109    7.968035e+06  5.307582e+06\n",
      "190 2000-07-12  0.656959    6.909205e+06  4.539064e+06\n",
      "309 2000-11-08  0.633626    7.016172e+06  4.445628e+06\n"
     ]
    }
   ],
   "source": [
    "# from glob import glob\n",
    "\n",
    "# 저장된 모델 로드\n",
    "clf, reg, meta = load_model(\"sep_two_stage_1996_2018.joblib\")\n",
    "\n",
    "bundle = {\"clf\": clf, \"reg\": reg, \"meta\": meta}\n",
    "\n",
    "year = 2000\n",
    "\n",
    "txt_list = [f\"./cme_data/univ{year}_01.txt\",\n",
    "            f\"./cme_data/univ{year}_02.txt\",\n",
    "            f\"./cme_data/univ{year}_03.txt\",\n",
    "            f\"./cme_data/univ{year}_04.txt\",\n",
    "            f\"./cme_data/univ{year}_05.txt\",\n",
    "            f\"./cme_data/univ{year}_06.txt\",\n",
    "            f\"./cme_data/univ{year}_07.txt\",\n",
    "            f\"./cme_data/univ{year}_08.txt\",\n",
    "            f\"./cme_data/univ{year}_09.txt\",\n",
    "            f\"./cme_data/univ{year}_10.txt\",\n",
    "            f\"./cme_data/univ{year}_11.txt\",\n",
    "            f\"./cme_data/univ{year}_12.txt\"]\n",
    "\n",
    "data_list = make_multiple_cme_txts(\n",
    "    txt_list,\n",
    "    year=year\n",
    ")\n",
    "\n",
    "pred = predict_sep_from_cme_daily(bundle, data_list)\n",
    "\n",
    "print(pred.head())\n",
    "print(pred.sort_values(\"p_event\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1faecc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv(\"pred_2000.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b21d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
